---
title: "AugMentor Learner Model"
output:
  pdf_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

```{r setup, include=FALSE}
# 1. Define the list of packages you need
pkgs <- c("tidyverse", "readxl", "ggplot2", "readr", "dplyr", "janitor","lubridate","purrr")

# 2. Check if they are installed; if not, install them
install_if_missing <- pkgs[!(pkgs %in% installed.packages()[, "Package"])]
if (length(install_if_missing)) install.packages(install_if_missing)

# This chunk won't appear in your final report, but it loads everything
library(tidyverse)
library(ggplot2)
library(readr)
library(dplyr)
library(janitor)
library(lubridate)
library(purrr)

# Optional: Set global options (e.g., hide all code warnings)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

Load the data in a dataframe
```{r}
# This tells R: "Look in the 'data' folder, then find the CSV"
df <- read.csv("data/Composite_data_W1.csv", na = c("", "n/a", "N/A"))

# 2. Keep everything from the first column up to "efficiency"
df_clean <- df %>% 
  select(1:Final_Score)%>%
  clean_names()

# Check the result
colnames(df_clean)
```
## Calculate Motivation or Engagement with SelfDetermination Theory

Follows methodology in [Cristea et al.](https://doi.org/10.1007/s11257-023-09374-x) which quantifies engagement/motivation according to the Self-Determination theory (SDT). Defines a `max_norm()` function to do 'Max Normalisation' as the methodlogy uses normalisation for standarisation of behavorial indicators.

```{r}

# --- helper functions ---
max_norm <- function(x) {
  #finds the highest number in the column. Ignore empty (NA) cells
  m <- suppressWarnings(max(x, na.rm = TRUE)) 
  #returns a column of zeros, if it fails
  if (!is.finite(m) || m <= 0) return(rep(0, length(x))) 
  #divide every entry in the column by the maximum value
  x / m
}

```

`nomralise_var()` creates a  Normalised score, by help of `max_norm()`, from a list of raw data and attaches it to the data frame. 

```{r}
nomralise_var <- function(df, vars, prefix, norm_func = max_norm) {
  #identify which columns are missing from raw data
  present <- intersect(vars, names(df))
  missing <- setdiff(vars, names(df))

  #list the missing columns from raw data
  message(prefix, " - missing vars: ",
          if (length(missing) == 0) "None" else paste(missing, collapse = ", "))

  #if none of the variables were found, exit
  if (length(present) == 0) {
    df[[prefix]] <- NA_real_
    return(list(df = df, used = character(0), missing = missing))
  }

  # create normalized indicator columns
  norm_cols <- paste0(prefix, "_", present, "_norm")
  for (i in seq_along(present)) {
    v <- present[i]
    df[[norm_cols[i]]] <- norm_func(df[[v]])
  }

  # construct = mean of normalized indicators
  df[[prefix]] <- rowMeans(df[, norm_cols, drop = FALSE], na.rm = TRUE)

  list(df = df, used = present, missing = missing, norm_cols = norm_cols)
}

```

### Autonomy
Calculates Autonomy component. [Cristea et al.](https://doi.org/10.1007/s11257-023-09374-x) only uses discrete (counting) data based behavioral indicators for general availability and applicability, for example number of clicks, number of visits, etc. `aut_vars` holds the variable to be used to calculate autonmy component. Ensure the name matches the ones in `df_clean`. 

Here, the following two indicators are used as prescribed by [Cristea et al.](https://doi.org/10.1007/s11257-023-09374-x)

* `no_attemptstoanswer_w1`: is the number of attempts a student makes to answer any questions during the week
* `no_resources_accessed`: is the number of views made by a student in all the learning content in this week

```{r}
aut_vars <- c("no_attemptstoanswer_w1", "no_resources_accessed")

res_aut <- nomralise_var(df_clean, aut_vars, prefix = "Aut")
df_clean <- res_aut$df
view(df_clean)
```

### Competence
Calculate competence similarly to Autonomy. `com_vars` holds the variable to be used to calculate competence component. Ensure the name matches the ones in `df_clean`.

Here, the following two indicators are used as prescribed by [Cristea et al.](https://doi.org/10.1007/s11257-023-09374-x)

* `no_revisits_b4quiz_w1`: is the number of revisits (after the first exposure) a student makes in all learning content (except quizzes)
* `no_analytics_viewed`: is the number of times an analytics page was viewed by a student

```{r}

com_vars <- c("no_revisits_b4quiz_w1", "no_analytics_viewed")
res_com <- nomralise_var(df_clean, com_vars, prefix = "Comp")
df_clean <- res_com$df
view(df_clean)
```

No **Relatedness** is calculated in this context as there was no relevant indicators. Cristea mostly uses social, collaborative indicators from forums etc. but in this context no such data was available.

## Calculate initial Memory Strength
Below, we will implement the learner model by first calculating the weights for each construct/indicator (how much each indicator impacts intial strength of memory)

### Fit a model to determine the with of each indicators 
The weight for each construct of the learner model is determined for the week using fractional regression which enables output between 0-1, which will later be used to calculate the intial strength
```{r}
df_weights <- df_clean %>%
  mutate(
    zAut    = as.numeric(scale(Aut)), #standarises these with zscores
    zComp   = as.numeric(scale(Comp)),
    zEffort = as.numeric(scale(effort_proxy)), #even though effort_proxy is already a zscore, decided to do it again for consistency
    quiz_prop = quiz01_score / 3, #bound the value to 0-1 for use in strength calculation
  )  %>%
  mutate(
    # keep away from exact 0/1 for logit stability
    quiz_prop = pmin(1 - 1e-6, pmax(1e-6, quiz_prop))
  )

#fractional logit glm: logit link guarantees the probability/strength of memory between [0,1] 
m_init <- glm(
  quiz_prop ~ zAut + zComp + zEffort,
  data = df_weights,
  family = quasibinomial(link = "logit")
)

summary(m_init)

```

### Calculate initial memory strength for each student
Memory Strength score for students based on the model above.

```{r}

df_weights <- df_weights %>%
  mutate(S0 = predict(m_init, newdata = ., type = "response"))
view(df_weights)

```

## Calculate Decay
Load the log file and filter events that fall between the first weekly exam and the final exam.

```{r}
#The date of the final exam
finalexam_cutoff <- ymd_hm("2020-12-16 00:00", tz = "Europe/Berlin")

#import the raw log file and clean it
df_log_w1 <- read_csv("data/log_data_W1.csv", na = c("", "n/a", "N/A")) %>%
  clean_names() %>%
  mutate(
    student_id = as.character(student_id),
    timestamp  = dmy_hm(time_sort, tz = "Europe/Berlin"),
    component  = tolower(component),
    event_name = tolower(event_name)
  ) %>%
  filter(!is.na(timestamp), timestamp < finalexam_cutoff) %>% #Remove any rows where the date or time is missing or incorrectly formatted
  filter(component %in% c("url", "file", "quiz")) #Remove any activity that happened after the exam started.

colnames(df_log_w1) #Check the names

#append the W1 quiz time stamp to the log for futher calculations
df_joined <- df_log_w1 %>%
  left_join(df_clean %>% select(student_id, time_w1_quizz), by = "student_id") %>%
  filter(!is.na(timestamp), !is.na(time_w1_quizz)) %>%
  filter(timestamp > time_w1_quizz, timestamp < finalexam_cutoff)

view(df_joined)
```

### Define revision study sessions
We mark each revisit of Week-1 materials and quiz after the first instance of Week1 quiz, defined by the event "Quiz attempt Submitted" and before the final exam which was held in 2020-12-16. Logs entries within `session_gap_mins` minutes of each other will be considered one learning session

```{r}
quiz_review_events <- tolower(c(
  "Quiz attempt reviewed",
  "Quiz attempt viewed",
  "Quiz attempt summary viewed",
  "Course module viewed"
))

# keep only “review” events
df_review_events <- df_joined %>%
  filter(
    component %in% c("url", "file") |
      (component == "quiz" & event_name %in% quiz_review_events)
  ) %>%
  arrange(student_id, timestamp) %>%
  mutate(
    # classify review type
    review_type = if_else(component == "quiz", "quiz", "resource")
  )

#if a day has passed since last visit, create a new session
session_gap_mins <- 1440

review_sessions <- df_review_events %>%
  group_by(student_id) %>%
  arrange(timestamp) %>%
  mutate(
    #Calculate the delta between the current row and the row immediately before it
    gap_mins = as.numeric(difftime(timestamp, lag(timestamp), units = "mins")),
    new_session = if_else(is.na(gap_mins) | gap_mins > session_gap_mins, 1L, 0L),
    session_id = cumsum(new_session)
  ) %>%
  group_by(student_id, session_id) %>%
  summarise(
    session_time = min(timestamp),
    # if any quiz review happened in the session, treat session as “quiz”, otherwise resource
    session_type = if_else(any(review_type == "quiz"), "quiz", "resource"),
    .groups = "drop"
  )

view(review_sessions)

```


### Decay

calculates strength of memory at the final exam based on explonential decay. Takes into consideration the time difference and number of revision

```{r}
#parameters (tune later)
HL_days <- 7          # half-life in days: strength halves every HL_days if no review
#as a introductory topic or relatively simple materials i have chosen to provide a high strenghening
b_quiz  <- 0.1       # boost after a quiz-type session
b_res   <- 0.1       # boost after a resource-type session, however, since students were not allowed to retake quizzes they are both same
eps     <- 1e-6       # clamp to avoid exact 0/1
exam_time <- finalexam_cutoff 

#simulator: returns strength at each revision
simulate_student <- function(student_id, quiz_time, S0, review_sessions, exam_time) {

  # clamp initial: ensures value is between 0-1
  S <- pmin(1 - eps, pmax(eps, S0))
  last_t <- quiz_time

  sess <- review_sessions %>%
    filter(student_id == !!student_id,
           session_time > quiz_time,
           session_time < exam_time) %>%
    arrange(session_time)

  out <- tibble()

  if (nrow(sess) > 0) {
    for (i in seq_len(nrow(sess))) {
      t_i <- sess$session_time[i]
      dt_days <- as.numeric(difftime(t_i, last_t, units = "days")) #the time gap since the study session

      # forgetting (exponential decay)
      S_before <- S * exp(-log(2) * dt_days / HL_days)

      # review boost to memory strength (saturating so it never exceeds 1)
      b <- if (sess$session_type[i] == "quiz") b_quiz else b_res
      S_after <- S_before + b * (1 - S_before) 

      out <- bind_rows(out, tibble(
        student_id      = student_id,
        revision_index  = i,
        revision_time   = t_i,
        gap_days        = dt_days,
        revision_type   = sess$session_type[i],
        strength_before = S_before,
        strength_after  = S_after
      ))

      S <- S_after
      last_t <- t_i
    }
  }

  # decay to exam
  dt_exam <- as.numeric(difftime(exam_time, last_t, units = "days"))
  S_exam <- S * exp(-log(2) * dt_exam / HL_days)

  out <- bind_rows(out, tibble(
    student_id      = student_id,
    revision_index  = ifelse(nrow(out) == 0, 0, max(out$revision_index) + 1),
    revision_time   = exam_time,
    gap_days        = dt_exam,
    revision_type   = "exam",
    strength_before = S_exam,
    strength_after  = S_exam
  ))

  out
}

# run for all students
df_base <- df_weights %>%
  transmute(
    student_id = as.character(student_id),
    quiz_time  = dmy_hm(time_w1_quizz, tz = "Europe/Berlin"),
    S0         = S0
  ) %>%
  filter(!is.na(quiz_time), !is.na(S0))

strength_timeline <- purrr::pmap_dfr(
  list(df_base$student_id, df_base$quiz_time, df_base$S0),
  ~simulate_student(..1, ..2, ..3, review_sessions, exam_time)
)

# retention at final exam (one row per student)
retention_at_exam <- strength_timeline %>%
  filter(revision_type == "exam") %>%
  transmute(student_id, S_exam = strength_after)

print(retention_at_exam, n = Inf)

```

### Compare the prediction aginst the observed grades
Takes the predicted memory strength `S_exam`for each student and compare it to their actual exam scores to judge accuracy

```{r}
# Pull the observed final exam score (adjust the column name if yours differs)
observed_final <- df_clean %>%
  transmute(
    student_id = as.character(student_id),
    Final_Score = final_score
  ) %>%
  filter(!is.na(Final_Score))

# Join predictions (S_exam: strength at final exam) with observed finals
eval_df <- retention_at_exam %>%
  left_join(observed_final, by = "student_id") %>%
  filter(!is.na(Final_Score))

print(eval_df, n = Inf)

# how well S_exam ranks students vs Final_Score
# measures the strength and direction between two ranked variables
spearman_rho <- cor(eval_df$S_exam, eval_df$Final_Score, method = "spearman")

# measures the strength and direction of a linear relationship between two continuous variable
pearson_r    <- cor(eval_df$S_exam, eval_df$Final_Score, method = "pearson")

cat("Spearman rho:", spearman_rho, "\n") 
cat("Pearson r:", pearson_r, "\n")

# Error metrics (need both on comparable scales)
eval_df <- eval_df %>%
  mutate(
    Final_minmax = (Final_Score - min(Final_Score)) / (max(Final_Score) - min(Final_Score)) # Normalize Final_Score
  )

mae  <- mean(abs(eval_df$S_exam - eval_df$Final_minmax))
rmse <- sqrt(mean((eval_df$S_exam - eval_df$Final_minmax)^2))

cat("MAE pre on grade scale:", mae, "\n") #calculates mean absolute error
cat("RMSE pre on grade scale:", rmse, "\n") #calculates root mean squared error

#keeps predictions within 0..4
eval_df <- eval_df %>%
  mutate(Final_01 = Final_Score / 4)

calib <- glm(Final_01 ~ S_exam, data = eval_df,
             family = quasibinomial(link = "logit"))
print(summary(calib))

# predicted grade (0..4)
eval_df <- eval_df %>%
  mutate(Final_hat_01 = predict(calib, type = "response"),
         Final_hat = 4 * Final_hat_01)

# error on grade scale (0..4)
mae_grade  <- mean(abs(eval_df$Final_hat - eval_df$Final_Score))
rmse_grade <- sqrt(mean((eval_df$Final_hat - eval_df$Final_Score)^2))


cat("MAE on grade scale:", mae_grade, "\n") #calculates mean absolute error
cat("RMSE on grade scale:", rmse_grade, "\n") #calculates root mean squared error


```

### Draw the plot
Fitted model vs actual grades

```{r}
# Plot
ggplot(eval_df, aes(x = S_exam, y = Final_Score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Predicted retention at exam (S_exam)", y = "Observed Final_Score")

```
### Check whats driving the drop
```{r}
mean(eval_df$Final_Score - 4*eval_df$S_exam)
```


When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
